{
  "url": "https://techcommunity.microsoft.com/blog/azure-ai-services-blog/model-context-protocol-mcp-integrating-azure-openai-for-enhanced-tool-integratio/4393788",
  "title": "Model Context Protocol (MCP): Integrating Azure OpenAI for Enhanced Tool Integration and Prompting",
  "capturedAt": "2025-05-22T02:19:23.589Z",
  "contentType": "text/html; charset=utf-8",
  "author": "mrajguru",
  "excerpt": "Model Context Protocol serves as a critical communication bridge between AI models and external systems, enabling AI assistants to interact directly with various services through a standardized interface. This protocol was designed to address the inherent limitations of standalone AI models by providing them with pathways to access real-time data, perform actions in external systems, and leverage specialized tools beyond their built-in capabilities. The fundamental architecture of MCP consists of client-server communication where the AI model (client) can send requests to specialized servers that handle specific service integrations, process these requests, and return formatted results that the AI can incorporate into its responses. This design pattern enables AI systems to maintain their core reasoning capabilities while extending their functional reach into practical applications that require interaction with external systems and databases\n\nMCP has the potential to function as a universal interface, think of it as the virtual / software version of USB-C for AI. Enabling seamless, secure and scalable data exchange between LLMs/AI Agents and external resources. MCP uses a&nbsp;client-server architecture where MCP hosts (AI applications) communicate with MCP servers (data/tool providers). Developers can use MCP to build reusable, modular connectors, with pre-built servers available for popular platforms, creating a community-driven ecosystem. MCP’s open-source nature encourages innovation, allowing developers to extend its capabilities while maintaining security through features like granular permissions. Ultimately, MCP aims to transform AI Agents from isolated chatbots into context-aware, interoperable systems deeply integrated into digital environments. Key elements from the Model Context Protocol:\n\nStandardization: MCP provides a standardized way for language models to interact with tools, promoting interoperability.\nCommunication Methods: Supports multiple communication methods, including STDIO and SSE, for flexibility in tool integration.\nTool Integration: Enables language models to use external tools, enhancing their functionality and applicability.\n\nHow Does It Work?\nMCP operates on a client-server architecture:\n\nMCP Hosts: These are the AI applications or interfaces, such as&nbsp; IDEs, or AI tools, that seek to access data through MCP. They initiate requests for data or actions.\nMCP Clients: These are protocol clients that maintain a one-to-one connection with MCP servers, acting as intermediaries to forward requests and responses.\nMCP Servers: Lightweight programs that expose specific capabilities through the MCP, connecting to local or remote data sources. Examples include servers for file systems, databases, or APIs, each advertising their capabilities for hosts to utilize.\nLocal Data Sources: These include the computer’s files, databases, and services that MCP servers can securely access, such as reading local documents or querying SQLite databases.\nRemote Services: External systems available over the internet, such as APIs, that MCP servers can connect to, enabling AI to interact with cloud-based tools or services.\n\nSource: https://x.com/minchoi/status/1900931746448756879\nImplementation\nlets try to implement a MCP client using Azure OpenAI with Chainlit and openai python library. By end of this blog you can use attach any MCP server to your client and start using with a simple user interface. So lets get started.\n&nbsp;\nFirst thing we need to ensure is our MCP tools are listed and loaded to our chainlit session. As you install any MCP server , you need to ensure that all the tools of those associated MCP servers are added to your session.&nbsp;\n&nbsp;\n.on_chat_start\nasync def start_chat():\n    client = ChatClient()\n    cl.user_session.set(\"messages\", [])\n    cl.user_session.set(\"system_prompt\", SYSTEM_PROMPT)\n\n@cl.on_mcp_connect\nasync def on_mcp(connection, session: ClientSession):\n    result = await session.list_tools()\n    tools = [{\n        \"name\": t.name,\n        \"description\": t.description,\n        \"parameters\": t.inputSchema,\n        } for t in result.tools]\n    \n    mcp_tools = cl.user_session.get(\"mcp_tools\", {})\n    mcp_tools[connection.name] = tools\n    cl.user_session.set(\"mcp_tools\", mcp_tools)\n&nbsp;\nNext thing we need to do is that we have to flatten the tools as the same will be passed to Azure OpenAI. In this case for each message we pass the loaded MCP server session tools into chat session after flattening it.&nbsp;\n&nbsp;\ndef flatten(xss):\n    return [x for xs in xss for x in xs]\n\n@cl.on_message\nasync def on_message(message: cl.Message):\n    mcp_tools = cl.user_session.get(\"mcp_tools\", {})\n    tools = flatten([tools for _, tools in mcp_tools.items()])\n    tools = [{\"type\": \"function\", \"function\": tool} for tool in tools]\n    \n    # Create a fresh client instance for each message\n    client = ChatClient()\n    # Restore conversation history\n    client.messages = cl.user_session.get(\"messages\", [])\n    \n    msg = cl.Message(content=\"\")\n    async for text in client.generate_response(human_input=message.content, tools=tools):\n        await msg.stream_token(text)\n    \n    # Update the stored messages after processing\n    cl.user_session.set(\"messages\", client.messages)\n&nbsp;\nNext I define a tool calling step which basically call the MCP session to execute the tool.\n&nbsp;\n.step(type=\"tool\") \nasync def call_tool(mcp_name, function_name, function_args):\n    try:\n        print(f\"Function Name: {function_name} Function Args: {function_args}\")\n        mcp_session, _ = cl.context.session.mcp_sessions.get(mcp_name)\n        func_response = await mcp_session.call_tool(function_name, function_args)\n    except Exception as e:\n        traceback.print_exc()\n        func_response = json.dumps({\"error\": str(e)})\n    return str(func_response.content)\n\n&nbsp;\nNext i define a chat client which basically can run as many tools in an iterative manner through for loop (No third party library), simple openai python client.&nbsp;\n&nbsp;\nimport json\nfrom mcp import ClientSession\nimport os\nimport re\nfrom aiohttp import ClientSession\nimport chainlit as cl\nfrom openai import AzureOpenAI, AsyncAzureOpenAI\nimport traceback\nfrom dotenv import load_dotenv\nload_dotenv(\"azure.env\")\n\nSYSTEM_PROMPT = \"you are a helpful assistant.\"\n\nclass ChatClient:\n    def __init__(self) -&gt; None:\n        self.deployment_name = os.environ[\"AZURE_OPENAI_MODEL\"]\n        self.client = AsyncAzureOpenAI(\n                azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n                api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n                api_version=\"2024-12-01-preview\",\n            )\n        self.messages = []\n        self.system_prompt = SYSTEM_PROMPT\n    async def process_response_stream(self, response_stream, tools, temperature=0):\n        \"\"\"\n        Recursively process response streams to handle multiple sequential function calls.\n        This function can call itself when a function call is completed to handle subsequent function calls.\n        \"\"\"\n        function_arguments = \"\"\n        function_name = \"\"\n        tool_call_id = \"\"\n        is_collecting_function_args = False\n        collected_messages = []\n        \n        try:\n            async for part in response_stream:\n                if part.choices == []:\n                    continue\n                delta = part.choices[0].delta\n                finish_reason = part.choices[0].finish_reason\n                \n                # Process assistant content\n                if delta.content:\n                    collected_messages.append(delta.content)\n                    yield delta.content\n                \n                # Handle tool calls\n                if delta.tool_calls:\n                    if len(delta.tool_calls) &gt; 0:\n                        tool_call = delta.tool_calls[0]\n                        \n                        # Get function name\n                        if tool_call.function.name:\n                            function_name = tool_call.function.name\n                            tool_call_id = tool_call.id\n                        \n                        # Process function arguments delta\n                        if tool_call.function.arguments:\n                            function_arguments += tool_call.function.arguments\n                            is_collecting_function_args = True\n                \n                # Check if we've reached the end of a tool call\n                if finish_reason == \"tool_calls\" and is_collecting_function_args:\n                    # Process the current tool call\n                    print(f\"function_name: {function_name} function_arguments: {function_arguments}\")\n                    function_args = json.loads(function_arguments)\n                    mcp_tools = cl.user_session.get(\"mcp_tools\", {})\n                    mcp_name = None\n                    for connection_name, session_tools in mcp_tools.items():\n                        if any(tool.get(\"name\") == function_name for tool in session_tools):\n                            mcp_name = connection_name\n                            break\n                    \n                    reply_to_customer = function_args.get('reply_to_customer')\n                    print(f\"reply_to_customer: {reply_to_customer}\")\n                    # Output any replies to the customer\n                    if reply_to_customer:\n                        tokens = re.findall(r'\\s+|\\w+|[^\\w\\s]', reply_to_customer)\n                        for token in tokens:\n                            yield token\n                    \n                    # Add the assistant message with tool call\n                    self.messages.append({\n                        \"role\": \"assistant\", \n                        \"content\": reply_to_customer,\n                        \"tool_calls\": [\n                            {\n                                \"id\": tool_call_id,\n                                \"function\": {\n                                    \"name\": function_name,\n                                    \"arguments\": function_arguments\n                                },\n                                \"type\": \"function\"\n                            }\n                        ]\n                    })\n                    func_response = await call_tool(mcp_name, function_name, function_args)\n                    # Add the tool response\n                    self.messages.append({\n                        \"tool_call_id\": tool_call_id,\n                        \"role\": \"tool\",\n                        \"name\": function_name,\n                        \"content\": func_response,\n                    })\n                    \n                    # Create a new stream to continue processing\n                    new_response = await self.client.chat.completions.create(\n                        model=self.deployment_name,\n                        messages=self.messages,\n                        tools=tools,\n                        parallel_tool_calls=False,\n                        stream=True,\n                        temperature=temperature\n                    )\n                    \n                    # Use a separate try block for recursive processing\n                    try:\n                        async for token in self.process_response_stream(new_response, tools, temperature):\n                            yield token\n                    except GeneratorExit:\n                        return\n                    return\n                \n                # Check if we've reached the end of assistant's response\n                if finish_reason == \"stop\":\n                    # Add final assistant message if there's content\n                    if collected_messages:\n                        final_content = ''.join([msg for msg in collected_messages if msg is not None])\n                        if final_content.strip():\n                            self.messages.append({\"role\": \"assistant\", \"content\": final_content})\n                    return\n        except GeneratorExit:\n            return\n        except Exception as e:\n            print(f\"Error in process_response_stream: {e}\")\n            traceback.print_exc()\n    \n    # Main entry point that uses the recursive function\n    async def generate_response(self, human_input, tools, temperature=0):\n        print(f\"human_input: {human_input}\")\n        self.messages.append({\"role\": \"user\", \"content\": human_input})\n        response_stream = await self.client.chat.completions.create(\n            model=self.deployment_name,\n            messages=self.messages,\n            tools=tools,\n            parallel_tool_calls=False,\n            stream=True,\n            temperature=temperature\n        )\n        try:\n        # Process the initial stream with our recursive function\n            async for token in self.process_response_stream(response_stream, tools, temperature):\n                yield token\n        except GeneratorExit:\n            return\nConclusion\nThe Model Context Protocol (MCP) is a pivotal development in AI integration, offering a standardized, open protocol that simplifies how AI models interact with external data and tools. Its client-server architecture, supported by JSON-RPC 2.0 and flexible transports, ensures efficient and secure communication, while its benefits of standardization, flexibility, security, efficiency, and scalability make it a valuable tool for developers. With diverse use cases like knowledge graph management, database queries, and API integrations, MCP is poised to unlock the full potential of AI applications, breaking down data silos and enhancing responsiveness. For those interested in exploring further, the rich documentation, SDKs, and community resources provide ample opportunities to engage with and contribute to this evolving standard.\n&nbsp;\nHere is the Githublink for end to end demo:\nThanks\nManoranjan Rajguru\nAI Global Black Belt, Asia\nhttps://www.linkedin.com/in/manoranjan-rajguru/",
  "siteName": "TECHCOMMUNITY.MICROSOFT.COM",
  "length": 14010,
  "source": {
    "type": "research_citation",
    "id": "fc0150fe6b530159",
    "topic": "The impact of MCP (Model Context Protocol) on AI system integration"
  }
}